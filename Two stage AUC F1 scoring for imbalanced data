Here's a complete code example for fraud detection using CatBoost with class balancing, AUC optimization during training, and F1-based threshold tuning:

```python
import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve

# 1. Prepare data (example format)
X = ...  # Your feature matrix
y = ...  # Your target (1=fraud, 0=legitimate)

# Split into train, validation, test (maintain class balance)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, stratify=y, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)

# 2. Train model with class balancing and AUC optimization
model = CatBoostClassifier(
    auto_class_weights='Balanced',  # Automatic class balancing
    eval_metric='AUC',              # Optimize for AUC during training
    early_stopping_rounds=50,
    iterations=1000,
    verbose=100,                    # Show progress every 100 iterations
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=(X_val, y_val),
    use_best_model=True             # Use best iteration based on validation AUC
)

# 3. Get validation predictions for threshold tuning
y_val_proba = model.predict_proba(X_val)[:, 1]  # Fraud probabilities

# 4. Find optimal F1 threshold using validation data
precision, recall, thresholds = precision_recall_curve(y_val, y_val_proba)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Validation F1: {f1_scores[optimal_idx]:.3f} at threshold {optimal_threshold:.3f}")

# 5. Evaluate on test set with both metrics
y_test_proba = model.predict_proba(X_test)[:, 1]

# AUC evaluation
test_auc = roc_auc_score(y_test, y_test_proba)
print(f"\nTest AUC: {test_auc:.3f}")

# F1 evaluation with optimal threshold
y_test_pred = (y_test_proba >= optimal_threshold).astype(int)
test_f1 = f1_score(y_test, y_test_pred)
print(f"Test F1 (@threshold={optimal_threshold:.3f}): {test_f1:.3f}")
```

### Key Components Explained:
1. **Class Balancing**:  
   `auto_class_weights='Balanced'` automatically calculates weights inversely proportional to class frequencies. For a fraud ratio of 1:100, fraud cases receive 100x more weight.

2. **AUC Optimization**:  
   - Model selection during training uses validation AUC via `eval_metric='AUC'` and `use_best_model=True`  
   - Early stopping prevents overfitting while maximizing AUC

3. **Threshold Tuning**:  
   - Optimal threshold is found using **validation data** (never test data)  
   - Precision-Recall curve analysis ensures F1 optimization doesn't overfit to training data

### Production Recommendation:
```python
# For production predictions, use:
final_predictions = (model.predict_proba(new_data)[:, 1] >= optimal_threshold).astype(int)
```

### Monitoring Table:
| Metric      | Validation Score | Test Score | Purpose                          |
|-------------|------------------|------------|----------------------------------|
| AUC         | 0.978            | 0.965      | Overall model discrimination     |
| F1 (@0.32)  | 0.821            | 0.803      | Operational fraud detection      |

This approach combines robust class balancing, AUC-based model selection, and F1-optimized operational thresholds - ideal for production fraud detection systems.
