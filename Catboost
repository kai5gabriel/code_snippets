import numpy as np
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer, confusion_matrix

# Your existing hyperparameter tuning code remains the same
# ...

# After finding the best hyperparameters and training the final model:
final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=100
)

# Create a Pool object for threshold selection
val_pool = Pool(X_val, y_val)

# Option 1: Automatically select threshold based on a target false positive rate
# For example, to target a 5% false positive rate:
# Correct usage:
optimal_threshold = select_threshold(model=final_model, data=val_pool, FPR=0.05)


# Option 2: Manually test different thresholds to find the optimal one
thresholds = np.arange(0.5, 0.99, 0.05)
results = []

for threshold in thresholds:
    # Set the threshold
    final_model.set_probability_threshold(threshold)
    
    # Get predictions with this threshold
    y_pred = final_model.predict(X_val)
    
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
    
    # Calculate metrics
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f_beta = fbeta_score(y_val, y_pred, beta=0.5)  # F0.5 score emphasizes precision
    
    # Store results
    results.append({
        'threshold': threshold,
        'tp': tp, 
        'fp': fp,
        'fn': fn, 
        'tn': tn,
        'precision': precision,
        'recall': recall,
        'f_beta': f_beta
    })
    
    print(f"Threshold: {threshold:.2f}, TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}, F0.5: {f_beta:.4f}")

# Find the threshold that maximizes F0.5 score (or another preferred metric)
best_result = max(results, key=lambda x: x['f_beta'])
print(f"\nRecommended threshold: {best_result['threshold']:.2f}")
print(f"TP: {best_result['tp']}, FP: {best_result['fp']}, Precision: {best_result['precision']:.4f}, Recall: {best_result['recall']:.4f}")

# Set the model to use this threshold for all future predictions
final_model.set_probability_threshold(best_result['threshold'])



----------------------------------------------------------------------------------------------------

import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer, roc_curve, precision_recall_curve, f1_score, auc
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# [Your existing code for parameter space definition]

# Keep your hyperparameter optimization as is
# [Your existing BayesSearchCV code]

# Train the final model with best parameters as you did before
# [Your existing final_model definition and training]

# After training the model, we'll add threshold optimization
# Get the predicted probabilities for the validation set
val_probs = final_model.predict_proba(X_val)[:, 1]

# Method 1: ROC Curve analysis
fpr, tpr, thresholds_roc = roc_curve(y_val, val_probs)
# Calculate J statistic (Youden's index) to find optimal threshold
J = tpr - fpr
optimal_idx = np.argmax(J)
optimal_threshold_roc = thresholds_roc[optimal_idx]

# Method 2: Precision-Recall Curve analysis
precision, recall, thresholds_pr = precision_recall_curve(y_val, val_probs)
# Find threshold that maximizes F1 score
f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])
optimal_idx_pr = np.argmax(f1_scores)
optimal_threshold_pr = thresholds_pr[optimal_idx_pr]

# Method 3: Use CatBoost's built-in threshold selection
from catboost.utils import select_threshold
# For a low false positive rate (e.g., 0.05 or 5%)
low_fpr_threshold = select_threshold(model=final_model, data=catboost.Pool(X_val, y_val), FPR=0.05)

# Compare results at different thresholds
thresholds_to_evaluate = {
    'Default (0.5)': 0.5,
    'ROC Optimal': optimal_threshold_roc,
    'PR Optimal': optimal_threshold_pr,
    'Low FPR (5%)': low_fpr_threshold,
    'Custom (0.7)': 0.7,  # You can adjust this value based on your risk tolerance
    'Custom (0.8)': 0.8,  # Even more conservative
    'Custom (0.9)': 0.9   # Very conservative
}

# Function to evaluate and display results for a threshold
def evaluate_threshold(threshold, name):
    # Apply threshold to get binary predictions
    y_pred = (val_probs >= threshold).astype(int)
    
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
    
    # Calculate metrics
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    f05 = (1 + 0.5**2) * precision * recall / ((0.5**2 * precision) + recall) if ((0.5**2 * precision) + recall) > 0 else 0
    
    print(f"\nThreshold: {name} ({threshold:.4f})")
    print(f"True Positives: {tp} | False Positives: {fp}")
    print(f"False Negatives: {fn} | True Negatives: {tn}")
    print(f"Precision: {precision:.4f} | Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f} | F0.5 Score: {f05:.4f}")
    
    return {'threshold': threshold, 'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn, 
            'precision': precision, 'recall': recall, 'f1': f1, 'f0.5': f05}

# Evaluate all thresholds
results = []
for name, threshold in thresholds_to_evaluate.items():
    results.append(evaluate_threshold(threshold, name))

# Find the threshold with the lowest false positives while maintaining at least 25 true positives
# (adjust the minimum true positives based on your requirements)
min_tp = 25  # Set your minimum acceptable true positive count
valid_results = [r for r in results if r['tp'] >= min_tp]
if valid_results:
    best_result = min(valid_results, key=lambda x: x['fp'])
    print(f"\nRecommended threshold to minimize FP while maintaining at least {min_tp} TP:")
    print(f"Threshold: {best_result['threshold']:.4f}")
    print(f"TP: {best_result['tp']} | FP: {best_result['fp']}")
    print(f"Precision: {best_result['precision']:.4f} | Recall: {best_result['recall']:.4f}")
    
    # Set this threshold in the model
    final_model.set_probability_threshold(best_result['threshold'])
else:
    print(f"\nNo threshold maintains at least {min_tp} true positives")

# Plot ROC Curve with different threshold points
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc(fpr, tpr):.4f})')
plt.plot([0, 1], [0, 1], 'k--')

# Mark different thresholds on the curve
for result in results:
    # Find the closest point on the ROC curve
    threshold = result['threshold']
    idx = np.abs(thresholds_roc - threshold).argmin()
    plt.plot(fpr[idx], tpr[idx], 'o', markersize=10)
    plt.annotate(f"{threshold:.2f}", (fpr[idx], tpr[idx]), xytext=(10, 10), 
                 textcoords='offset points')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve with Different Thresholds')
plt.legend(loc='lower right')
plt.grid(True)
plt.savefig('roc_curve_thresholds.png')
plt.show()
------------------------------------------------------------------------------------------







import numpy as np
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer, f1_score, confusion_matrix

# Define the F1 scorer (equal weight to precision and recall)
f1_scorer = make_scorer(fbeta_score, beta=1)

# Define the parameter search space.
# The scale_pos_weight range is set around the expected 1:50000 imbalance.
param_space = {
    'scale_pos_weight': Real(30000, 100000, prior='log-uniform'),
    'depth': Integer(3, 10),
    'learning_rate': Real(0.005, 0.2, prior='log-uniform'),
    'l2_leaf_reg': Real(1, 100, prior='log-uniform'),
    'random_strength': Real(0.1, 10, prior='log-uniform'),
    'bootstrap_type': Categorical(['Bayesian', 'Bernoulli', 'MVS']),
    'rsm': Real(0.1, 0.8),
    'min_data_in_leaf': Integer(1, 100),
    'subsample': Real(0.1, 0.8),
}

# Set up the Bayesian hyperparameter tuner.
# We remove auto_class_weights so that scale_pos_weight is fully tuned.
opt = BayesSearchCV(
    estimator=CatBoostClassifier(
        loss_function='Logloss',
        iterations=10000,  # Large maximum; early stopping will prevent overtraining.
        random_seed=42,
        thread_count=-1,
        verbose=100,
    ),
    search_spaces=param_space,
    scoring=f1_scorer,
    cv=3,
    n_iter=40,
    n_points=3,
    refit=False,
    verbose=2
)

# Perform a stratified split so that class proportions are maintained.
X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# Fit the BayesSearchCV optimizer with early stopping.
opt.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=False
)

print("Best parameters found:", opt.best_params_)

# Create the final model using the best parameters.
final_params = opt.best_params_.copy()
final_params['iterations'] = 10000  # Set a high maximum; early stopping will manage training length.

final_model = CatBoostClassifier(
    loss_function='Logloss',
    random_seed=42,
    thread_count=-1,
    verbose=100,
    **final_params
)

# Train the final model with early stopping on the training set.
final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=100
)

# ---------------------
# Threshold Optimization
# ---------------------
# In a fraud detection scenario, a default 0.5 classification threshold may not be optimal.
# We evaluate multiple thresholds on the validation set based on increased F1 score.
pred_val_probs = final_model.predict_proba(X_val)[:, 1]

thresholds = np.linspace(0.1, 0.9, 17)  # Test thresholds from 0.1 to 0.9
best_threshold = 0.5
best_f1 = 0

for thr in thresholds:
    y_pred_thr = (pred_val_probs >= thr).astype(int)
    score = f1_score(y_val, y_pred_thr)
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred_thr).ravel()
    print(f"Threshold: {thr:.2f} | F1: {score:.4f} | TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")
    
    if score > best_f1:
        best_f1 = score
        best_threshold = thr

print("Optimal probability threshold based on F1 score:", best_threshold)

# Set the chosen threshold in the final model for future predictions.
final_model.set_probability_threshold(best_threshold)

# Final predictions on the validation set using the optimized threshold.
y_pred_final = final_model.predict(X_val)
print("Confusion Matrix with optimized threshold:")
print(confusion_matrix(y_val, y_pred_final))
