import numpy as np
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer, confusion_matrix

# Your existing hyperparameter tuning code remains the same
# ...

# After finding the best hyperparameters and training the final model:
final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=100
)

# Create a Pool object for threshold selection
val_pool = Pool(X_val, y_val)

# Option 1: Automatically select threshold based on a target false positive rate
# For example, to target a 5% false positive rate:
optimal_threshold = final_model.select_threshold(data=val_pool, FPR=0.05)

# Option 2: Manually test different thresholds to find the optimal one
thresholds = np.arange(0.5, 0.99, 0.05)
results = []

for threshold in thresholds:
    # Set the threshold
    final_model.set_probability_threshold(threshold)
    
    # Get predictions with this threshold
    y_pred = final_model.predict(X_val)
    
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
    
    # Calculate metrics
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f_beta = fbeta_score(y_val, y_pred, beta=0.5)  # F0.5 score emphasizes precision
    
    # Store results
    results.append({
        'threshold': threshold,
        'tp': tp, 
        'fp': fp,
        'fn': fn, 
        'tn': tn,
        'precision': precision,
        'recall': recall,
        'f_beta': f_beta
    })
    
    print(f"Threshold: {threshold:.2f}, TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}, F0.5: {f_beta:.4f}")

# Find the threshold that maximizes F0.5 score (or another preferred metric)
best_result = max(results, key=lambda x: x['f_beta'])
print(f"\nRecommended threshold: {best_result['threshold']:.2f}")
print(f"TP: {best_result['tp']}, FP: {best_result['fp']}, Precision: {best_result['precision']:.4f}, Recall: {best_result['recall']:.4f}")

# Set the model to use this threshold for all future predictions
final_model.set_probability_threshold(best_result['threshold'])



----------------------------------------------------------------------------------------------------

import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer, roc_curve, precision_recall_curve, f1_score, auc
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# [Your existing code for parameter space definition]

# Keep your hyperparameter optimization as is
# [Your existing BayesSearchCV code]

# Train the final model with best parameters as you did before
# [Your existing final_model definition and training]

# After training the model, we'll add threshold optimization
# Get the predicted probabilities for the validation set
val_probs = final_model.predict_proba(X_val)[:, 1]

# Method 1: ROC Curve analysis
fpr, tpr, thresholds_roc = roc_curve(y_val, val_probs)
# Calculate J statistic (Youden's index) to find optimal threshold
J = tpr - fpr
optimal_idx = np.argmax(J)
optimal_threshold_roc = thresholds_roc[optimal_idx]

# Method 2: Precision-Recall Curve analysis
precision, recall, thresholds_pr = precision_recall_curve(y_val, val_probs)
# Find threshold that maximizes F1 score
f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])
optimal_idx_pr = np.argmax(f1_scores)
optimal_threshold_pr = thresholds_pr[optimal_idx_pr]

# Method 3: Use CatBoost's built-in threshold selection
from catboost.utils import select_threshold
# For a low false positive rate (e.g., 0.05 or 5%)
low_fpr_threshold = select_threshold(model=final_model, data=catboost.Pool(X_val, y_val), FPR=0.05)

# Compare results at different thresholds
thresholds_to_evaluate = {
    'Default (0.5)': 0.5,
    'ROC Optimal': optimal_threshold_roc,
    'PR Optimal': optimal_threshold_pr,
    'Low FPR (5%)': low_fpr_threshold,
    'Custom (0.7)': 0.7,  # You can adjust this value based on your risk tolerance
    'Custom (0.8)': 0.8,  # Even more conservative
    'Custom (0.9)': 0.9   # Very conservative
}

# Function to evaluate and display results for a threshold
def evaluate_threshold(threshold, name):
    # Apply threshold to get binary predictions
    y_pred = (val_probs >= threshold).astype(int)
    
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
    
    # Calculate metrics
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    f05 = (1 + 0.5**2) * precision * recall / ((0.5**2 * precision) + recall) if ((0.5**2 * precision) + recall) > 0 else 0
    
    print(f"\nThreshold: {name} ({threshold:.4f})")
    print(f"True Positives: {tp} | False Positives: {fp}")
    print(f"False Negatives: {fn} | True Negatives: {tn}")
    print(f"Precision: {precision:.4f} | Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f} | F0.5 Score: {f05:.4f}")
    
    return {'threshold': threshold, 'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn, 
            'precision': precision, 'recall': recall, 'f1': f1, 'f0.5': f05}

# Evaluate all thresholds
results = []
for name, threshold in thresholds_to_evaluate.items():
    results.append(evaluate_threshold(threshold, name))

# Find the threshold with the lowest false positives while maintaining at least 25 true positives
# (adjust the minimum true positives based on your requirements)
min_tp = 25  # Set your minimum acceptable true positive count
valid_results = [r for r in results if r['tp'] >= min_tp]
if valid_results:
    best_result = min(valid_results, key=lambda x: x['fp'])
    print(f"\nRecommended threshold to minimize FP while maintaining at least {min_tp} TP:")
    print(f"Threshold: {best_result['threshold']:.4f}")
    print(f"TP: {best_result['tp']} | FP: {best_result['fp']}")
    print(f"Precision: {best_result['precision']:.4f} | Recall: {best_result['recall']:.4f}")
    
    # Set this threshold in the model
    final_model.set_probability_threshold(best_result['threshold'])
else:
    print(f"\nNo threshold maintains at least {min_tp} true positives")

# Plot ROC Curve with different threshold points
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc(fpr, tpr):.4f})')
plt.plot([0, 1], [0, 1], 'k--')

# Mark different thresholds on the curve
for result in results:
    # Find the closest point on the ROC curve
    threshold = result['threshold']
    idx = np.abs(thresholds_roc - threshold).argmin()
    plt.plot(fpr[idx], tpr[idx], 'o', markersize=10)
    plt.annotate(f"{threshold:.2f}", (fpr[idx], tpr[idx]), xytext=(10, 10), 
                 textcoords='offset points')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve with Different Thresholds')
plt.legend(loc='lower right')
plt.grid(True)
plt.savefig('roc_curve_thresholds.png')
plt.show()
------------------------------------------------------------------------------------------







import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer

# Define the search space for CatBoost
param_space = {
    'scale_pos_weight': Real(30000, 100000, prior='log-uniform'),  # adjusted for 1:50000 imbalance
    'depth': Integer(3, 10),  # equivalent to max_depth in LightGBM
    'learning_rate': Real(0.005, 0.2, prior='log-uniform'),
    'l2_leaf_reg': Real(1, 100, prior='log-uniform'),  # equivalent to reg_lambda
    'random_strength': Real(0.1, 10, prior='log-uniform'),  # randomness parameter
    'bootstrap_type': Categorical(['Bayesian', 'Bernoulli', 'MVS']),  # sampling strategy
    'rsm': Real(0.1, 0.8),  # column sampling, similar to colsample_bytree
    'min_data_in_leaf': Integer(1, 100),  # similar to min_child_samples
    'subsample': Real(0.1, 0.8),  # row subsampling
}

# Use F0.5 scorer to emphasize precision over recall
f05_scorer = make_scorer(fbeta_score, beta=0.5)

# Set up the BayesSearchCV object with CatBoostClassifier
opt = BayesSearchCV(
    estimator=CatBoostClassifier(
        loss_function='Logloss',  # Binary classification loss
        iterations=10000,  # Maximum number of trees (will use early stopping)
        random_seed=42,
        thread_count=-1,  # Use all CPU cores
        verbose=100,  # Print progress every 100 iterations
        auto_class_weights='Balanced'  # Handle class imbalance
    ),
    search_spaces=param_space,
    scoring=f05_scorer,  # Using F0.5 scorer
    cv=3,
    n_iter=30,
    n_points=3,
    refit=False,
    verbose=2
)

# Perform stratified split to preserve class proportions
X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# Fit the Bayesian optimizer with early stopping
# Note: CatBoost has early_stopping_rounds built into fit method (different from LightGBM)
opt.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=False  # Set to False as we already set verbose in the estimator
)

print("Best parameters found:", opt.best_params_)

# To create final model with best parameters:
final_params = opt.best_params_.copy()
final_params['iterations'] = 10000  # Explicitly set number of trees

final_model = CatBoostClassifier(
    loss_function='Logloss',
    random_seed=42,
    thread_count=-1,
    verbose=100,
    auto_class_weights='Balanced',
    **final_params
)

# Train the final model with early stopping
final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=100
)
