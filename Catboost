import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer

# Define the search space for CatBoost
param_space = {
    'scale_pos_weight': Real(30000, 100000, prior='log-uniform'),  # adjusted for 1:50000 imbalance
    'depth': Integer(3, 10),  # equivalent to max_depth in LightGBM
    'learning_rate': Real(0.005, 0.2, prior='log-uniform'),
    'l2_leaf_reg': Real(1, 100, prior='log-uniform'),  # equivalent to reg_lambda
    'random_strength': Real(0.1, 10, prior='log-uniform'),  # randomness parameter
    'bootstrap_type': Categorical(['Bayesian', 'Bernoulli', 'MVS']),  # sampling strategy
    'rsm': Real(0.1, 0.8),  # column sampling, similar to colsample_bytree
    'min_data_in_leaf': Integer(1, 100),  # similar to min_child_samples
    'subsample': Real(0.1, 0.8),  # row subsampling
}

# Use F0.5 scorer to emphasize precision over recall
f05_scorer = make_scorer(fbeta_score, beta=0.5)

# Set up the BayesSearchCV object with CatBoostClassifier
opt = BayesSearchCV(
    estimator=CatBoostClassifier(
        loss_function='Logloss',  # Binary classification loss
        iterations=10000,  # Maximum number of trees (will use early stopping)
        random_seed=42,
        thread_count=-1,  # Use all CPU cores
        verbose=100,  # Print progress every 100 iterations
        auto_class_weights='Balanced'  # Handle class imbalance
    ),
    search_spaces=param_space,
    scoring=f05_scorer,  # Using F0.5 scorer
    cv=3,
    n_iter=30,
    n_points=3,
    refit=False,
    verbose=2
)

# Perform stratified split to preserve class proportions
X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# Fit the Bayesian optimizer with early stopping
# Note: CatBoost has early_stopping_rounds built into fit method (different from LightGBM)
opt.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=False  # Set to False as we already set verbose in the estimator
)

print("Best parameters found:", opt.best_params_)

# To create final model with best parameters:
final_params = opt.best_params_.copy()
final_params['iterations'] = 10000  # Explicitly set number of trees

final_model = CatBoostClassifier(
    loss_function='Logloss',
    random_seed=42,
    thread_count=-1,
    verbose=100,
    auto_class_weights='Balanced',
    **final_params
)

# Train the final model with early stopping
final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=20,
    verbose=100
)
