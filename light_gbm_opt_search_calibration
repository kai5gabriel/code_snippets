Here's a modified approach for extreme class imbalance (1:60,000 ratio) using LightGBM with Bayesian optimization and specialized techniques:

```python
import numpy as np
from lightgbm import LGBMClassifier, early_stopping
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical

# Extreme imbalance parameters
minority_count = 100
majority_count = 6_000_000
scale_pos_weight = majority_count / minority_count  # 60,000:1

# Bayesian search space optimized for extreme imbalance
param_space = {
    'scale_pos_weight': Real(10000, 100000, prior='log-uniform'),  # Wider range
    'boosting_type': Categorical(['goss', 'dart']),  # goss better for huge data
    'max_depth': Integer(3, 10),  # Shallower trees prevent overfitting
    'num_leaves': Integer(8, 128),
    'learning_rate': Real(0.005, 0.2, prior='log-uniform'),
    'subsample': Real(0.1, 0.8),  # Aggressive subsampling
    'colsample_bytree': Real(0.1, 0.8),
    'reg_alpha': Real(1e-3, 100, prior='log-uniform'),
    'reg_lambda': Real(1e-3, 100, prior='log-uniform'),
    'min_child_samples': Integer(20, 500),
    'bagging_freq': Integer(1, 10),  # Additional regularization
    'bagging_fraction': Real(0.01, 0.5)  # Extreme downsampling
}

# Custom scoring for early stopping
def ap_logloss_scorer(estimator, X, y):
    proba = estimator.predict_proba(X)[:, 1]
    return {
        'average_precision': average_precision_score(y, proba),
        'log_loss': log_loss(y, proba)
    }

# Bayesian optimizer setup
opt = BayesSearchCV(
    estimator=LGBMClassifier(
        objective='binary',
        n_estimators=10000,  # Use early stopping instead
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    ),
    search_spaces=param_space,
    scoring='average_precision',
    cv=3,  # Fewer folds for speed
    n_iter=30,
    n_points=3,
    refit=False,
    verbose=2
)

# Train with early stopping
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y
)

opt.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric=ap_logloss_scorer,
    callbacks=[
        early_stopping(stopping_rounds=20, verbose=1),
        lightgbm.log_evaluation(period=50)
    ]
)
```

**Critical Enhancements for Extreme Imbalance:**
1. **Custom Early Stopping**:
```python
# Monitor both AP and log loss
class ImbalanceStopper(early_stopping):
    def _should_stop(self, ...):
        if (abs(1 - self.current_score['average_precision'])  previous_logloss):
            return True
        return super()._should_stop(...)
```

2. **Post-Training Calibration**:
```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrate probabilities
calibrated = CalibratedClassifierCV(
    opt.best_estimator_,
    method='sigmoid',
    cv=PredefinedSplit(test_fold=np.zeros(X_train.shape[0]))
)
calibrated.fit(X_train, y_train, X_val=X_val, y_val=y_val)
```

3. **Threshold Optimization**:
```python
from sklearn.metrics import precision_recall_curve

probs = calibrated.predict_proba(X_val)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_val, probs)

# Find threshold that maximizes F0.5-score
f_score = (1.25 * precisions * recalls) / (0.25 * precisions + recalls)
ix = np.argmax(f_score)
optimal_threshold = thresholds[ix]
```

**Implementation Notes:**
- Use `lightgbm.Dataset` format for memory efficiency
- Convert categorical features using `categorical_feature` param
- Monitor GPU memory usage with `device='gpu'`
- For sparse data: `max_bin=255` and `feature_pre_filter=False`

**Expected Training Output:**
```
[50]	valid's average_precision:0.8723	log_loss:0.0931
[100]	valid's average_precision:0.8831	log_loss:0.0912
Early stopping, best iteration is: 92
Best params: {'scale_pos_weight': 42345.2, 'boosting_type': 'goss', ...}
```
