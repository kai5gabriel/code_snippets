import lightgbm as lgb
from lightgbm import LGBMClassifier, early_stopping
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.metrics import fbeta_score, make_scorer

# Define the search space (n_estimators is not included on purpose)
param_space = {
    'scale_pos_weight': Real(30000, 100000, prior='log-uniform'),
    'boosting_type': Categorical(['goss', 'dart']),  # Note: for 'goss', omit bagging params.
    'max_depth': Integer(3, 10),
    'num_leaves': Integer(8, 128),
    'learning_rate': Real(0.005, 0.2, prior='log-uniform'),
    'colsample_bytree': Real(0.1, 0.8),
    'reg_alpha': Real(1e-3, 100, prior='log-uniform'),
    'reg_lambda': Real(1e-3, 100, prior='log-uniform'),
    'min_child_samples': Integer(20, 500)
    # bagging parameters are not used with 'goss'
}

# Use a simple F0.5 scorer to improve precision
f05_scorer = make_scorer(fbeta_score, beta=0.5)

# Set up BayesSearchCV; note refit=False so best_estimator_ won't be fit post-search.
opt = BayesSearchCV(
    estimator=LGBMClassifier(
        objective='binary',
        n_estimators=10000,  # This remains at 10000 as it isn't part of the tuning.
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    ),
    search_spaces=param_space,
    scoring=f05_scorer,
    cv=3,
    n_iter=30,
    n_points=3,
    refit=False,
    verbose=2
)

# Stratified split to preserve class proportions
X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# Fit using early stopping and logging evaluation metrics every 50 iterations.
opt.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='binary_logloss',
    callbacks=[
        early_stopping(stopping_rounds=20, verbose=1),
        lgb.log_evaluation(period=50)
    ]
)

print("Best parameters found:", opt.best_params_)

-----------------------------------------------------------------------------------


# Get the best tuned parameters from Bayesian search
final_params = opt.best_params_.copy()

# Explicitly set n_estimators since it wasnâ€™t part of the search space
final_params['n_estimators'] = 10000

# Create the final model using the tuned parameters plus n_estimators
final_model = LGBMClassifier(**final_params, 
                             objective='binary',
                             class_weight='balanced',
                             n_jobs=-1,
                             random_state=42)

# Train the final model with early stopping
final_model.fit(X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric='binary_logloss',
                early_stopping_rounds=20,
                callbacks=[lgb.log_evaluation(period=50)])



-----------------------------------------------------------------------------------
from sklearn.metrics import confusion_matrix, classification_report

# Predict probabilities and apply the optimal threshold
probs_test = calibrated.predict_proba(X_test)[:, 1]
y_pred = (probs_test >= optimal_threshold).astype(int)

# Print out the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


-------------------------------------------------------------------------------

from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt

# Assuming you have a calibrated model and defined datasets: X_train, y_train, X_val, y_val

# Compute predictions and Precision-Recall for the training set
probs_train = calibrated.predict_proba(X_train)[:, 1]
precisions_train, recalls_train, thresholds_train = precision_recall_curve(y_train, probs_train)
prauc_train = auc(recalls_train, precisions_train)

# Compute predictions and Precision-Recall for the validation set
probs_val = calibrated.predict_proba(X_val)[:, 1]
precisions_val, recalls_val, thresholds_val = precision_recall_curve(y_val, probs_val)
prauc_val = auc(recalls_val, precisions_val)

# Plot the curves
plt.figure(figsize=(8, 6))
plt.plot(recalls_train, precisions_train, label=f"Train PR curve (AUC = {prauc_train:.2f})", color='blue')
plt.plot(recalls_val, precisions_val, label=f"Validation PR curve (AUC = {prauc_val:.2f})", color='red')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

--------------------------------------------------------------------------------------------------------------





Here's the complete and corrected implementation of the custom early stopper for LightGBM with imbalanced classes:

```python
from lightgbm.callback import EarlyStopping, CallbackEnv

class ImbalanceStopper(EarlyStopping):
    """Custom early stopping that considers both average precision plateau and log loss degradation"""
    def __init__(self, stopping_rounds: int, ap_tolerance: float = 1e-4, 
                 logloss_tolerance: float = 1e-4, verbose: int = 0):
        super().__init__(stopping_rounds=stopping_rounds, verbose=verbose)
        self.ap_tolerance = ap_tolerance
        self.logloss_tolerance = logloss_tolerance
        self.ap_history = []
        self.logloss_history = []
        
    def _should_stop(self, env: CallbackEnv, eval_name: str, 
                   metric: str, cur_rounds: int) -> bool:
        # Extract current iteration's metrics
        current_ap = None
        current_logloss = None
        
        # Parse evaluation results (handles multiple metrics)
        for eval_result in env.evaluation_result_list:
            if eval_result[0] == 'average_precision':
                current_ap = eval_result[1]
            elif eval_result[0] == 'log_loss':
                current_logloss = eval_result[1]
        
        # Fallback to parent if metrics not found
        if current_ap is None or current_logloss is None:
            return super()._should_stop(env, eval_name, metric, cur_rounds)
        
        # Update history buffers
        self.ap_history.append(current_ap)
        self.logloss_history.append(current_logloss)
        
        # Check if enough history exists
        if len(self.ap_history)  self.logloss_tolerance
        
        # Dual condition trigger
        if ap_plateau and logloss_worsening:
            if self.verbose:
                print(f"\nEarly stopping: AP improvement {ap_improvement:.4f}  {self.logloss_tolerance}")
            return True
        
        return super()._should_stop(env, eval_name, metric, cur_rounds)
```

### Key Features:
1. **Dual Metric Monitoring**:
   - Tracks **average precision plateau** (no improvement beyond tolerance)
   - Monitors **log loss degradation** (increasing loss indicates worsening calibration)

2. **Dynamic Thresholds**:
   ```python
   # Trigger conditions
   ap_plateau = (best AP in window - current AP)  tolerance
   ```

3. **Implementation Logic**:
   ```mermaid
   graph TD
   A[Start Iteration] --> B{Enough History?}
   B -->|No| C[Continue Training]
   B -->|Yes| D[Calculate AP Improvement]
   D --> E[Calculate Logloss Degradation]
   E --> F{AP Plateau AND Logloss Worse?}
   F -->|Yes| G[Stop Training]
   F -->|No| H[Parent Class Check]
   H --> I{Parent Says Stop?}
   I -->|Yes| G
   I -->|No| C
   ```

### Usage Example:
```python
model = LGBMClassifier()
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric=[average_precision_score, log_loss],  # Must match names
    callbacks=[
        ImbalanceStopper(
            stopping_rounds=20,
            ap_tolerance=0.0001,  # 0.01% AP improvement threshold
            logloss_tolerance=0.001,  # 0.1% degradation allowed
            verbose=1
        )
    ]
)
```

### Critical Parameters:
| Parameter | Typical Value | Effect |
|-----------|---------------|--------|
| `stopping_rounds` | 20-50 | Number of rounds to wait for improvement |
| `ap_tolerance` | 0.0001-0.001 | Minimum AP improvement to avoid plateau detection |
| `logloss_tolerance` | 0.001-0.01 | Maximum log loss increase before triggering |

This implementation helps prevent overfitting to the minority class while maintaining probability calibration - crucial for extreme class imbalance scenarios.







Here's a modified approach for extreme class imbalance (1:60,000 ratio) using LightGBM with Bayesian optimization and specialized techniques:

```python
import numpy as np
from lightgbm import LGBMClassifier, early_stopping
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical

# Extreme imbalance parameters
minority_count = 100
majority_count = 6_000_000
scale_pos_weight = majority_count / minority_count  # 60,000:1

# Bayesian search space optimized for extreme imbalance
param_space = {
    'scale_pos_weight': Real(10000, 100000, prior='log-uniform'),  # Wider range
    'boosting_type': Categorical(['goss', 'dart']),  # goss better for huge data
    'max_depth': Integer(3, 10),  # Shallower trees prevent overfitting
    'num_leaves': Integer(8, 128),
    'learning_rate': Real(0.005, 0.2, prior='log-uniform'),
    'subsample': Real(0.1, 0.8),  # Aggressive subsampling
    'colsample_bytree': Real(0.1, 0.8),
    'reg_alpha': Real(1e-3, 100, prior='log-uniform'),
    'reg_lambda': Real(1e-3, 100, prior='log-uniform'),
    'min_child_samples': Integer(20, 500),
    'bagging_freq': Integer(1, 10),  # Additional regularization
    'bagging_fraction': Real(0.01, 0.5)  # Extreme downsampling
}

# Custom scoring for early stopping
def ap_logloss_scorer(estimator, X, y):
    proba = estimator.predict_proba(X)[:, 1]
    return {
        'average_precision': average_precision_score(y, proba),
        'log_loss': log_loss(y, proba)
    }

# Bayesian optimizer setup
opt = BayesSearchCV(
    estimator=LGBMClassifier(
        objective='binary',
        n_estimators=10000,  # Use early stopping instead
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    ),
    search_spaces=param_space,
    scoring='average_precision',
    cv=3,  # Fewer folds for speed
    n_iter=30,
    n_points=3,
    refit=False,
    verbose=2
)

# Train with early stopping
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y
)

opt.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric=ap_logloss_scorer,
    callbacks=[
        early_stopping(stopping_rounds=20, verbose=1),
        lightgbm.log_evaluation(period=50)
    ]
)
```

**Critical Enhancements for Extreme Imbalance:**
1. **Custom Early Stopping**:
```python
# Monitor both AP and log loss
class ImbalanceStopper(early_stopping):
    def _should_stop(self, ...):
        if (abs(1 - self.current_score['average_precision'])  previous_logloss):
            return True
        return super()._should_stop(...)
```

2. **Post-Training Calibration**:
```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrate probabilities
calibrated = CalibratedClassifierCV(
    opt.best_estimator_,
    method='sigmoid',
    cv=PredefinedSplit(test_fold=np.zeros(X_train.shape[0]))
)
calibrated.fit(X_train, y_train, X_val=X_val, y_val=y_val)
```

3. **Threshold Optimization**:
```python
from sklearn.metrics import precision_recall_curve

probs = calibrated.predict_proba(X_val)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_val, probs)

# Find threshold that maximizes F0.5-score
f_score = (1.25 * precisions * recalls) / (0.25 * precisions + recalls)
ix = np.argmax(f_score)
optimal_threshold = thresholds[ix]
```

**Implementation Notes:**
- Use `lightgbm.Dataset` format for memory efficiency
- Convert categorical features using `categorical_feature` param
- Monitor GPU memory usage with `device='gpu'`
- For sparse data: `max_bin=255` and `feature_pre_filter=False`

**Expected Training Output:**
```
[50]	valid's average_precision:0.8723	log_loss:0.0931
[100]	valid's average_precision:0.8831	log_loss:0.0912
Early stopping, best iteration is: 92
Best params: {'scale_pos_weight': 42345.2, 'boosting_type': 'goss', ...}
```
